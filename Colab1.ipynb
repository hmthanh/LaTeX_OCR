{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e11490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from entmax import entmax_bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ffda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "from collections import namedtuple\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from entmax import entmax15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14b2576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from x_transformers import *\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "# from x_transformers.autoregressive_wrapper import AutoregressiveWrapper, top_k, top_p, entmax, ENTMAX_ALPHA\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from timm.models.vision_transformer_hybrid import HybridEmbed\n",
    "from timm.models.resnetv2 import ResNetV2\n",
    "from timm.models.layers import StdConv2dSame\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4071a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas.io.clipboard as clipboard\n",
    "from PIL import ImageGrab\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from munch import Munch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from timm.models.resnetv2 import ResNetV2\n",
    "from timm.models.layers import StdConv2dSame\n",
    "from dataset.dataset import test_transform\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f00448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# nucleus\n",
    "\n",
    "def top_p(logits, thres = 0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cum_probs > (1 - thres)\n",
    "    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "    sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "    sorted_logits[sorted_indices_to_remove] = float('-inf')\n",
    "    return sorted_logits.scatter(1, sorted_indices, sorted_logits)\n",
    "\n",
    "# topk\n",
    "\n",
    "def top_k(logits, thres = 0.9):\n",
    "    k = ceil((1 - thres) * logits.shape[-1])\n",
    "    val, ind = torch.topk(logits, k)\n",
    "    probs = torch.full_like(logits, float('-inf'))\n",
    "    probs.scatter_(1, ind, val)\n",
    "    return probs\n",
    "\n",
    "# top_a\n",
    "\n",
    "def top_a(logits, min_p_pow=2.0, min_p_ratio=0.02):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    limit = torch.pow(torch.max(probs), min_p_pow) * min_p_ratio\n",
    "    logits[probs < limit] = -float(\"Inf\")\n",
    "    logits[probs >= limit] = 1\n",
    "    return logits\n",
    "\n",
    "# entmax\n",
    "\n",
    "ENTMAX_ALPHA = 1.3\n",
    "entmax = entmax_bisect\n",
    "\n",
    "class AutoregressiveWrapper(nn.Module):\n",
    "    def __init__(self, net, ignore_index = -100, pad_value = 0):\n",
    "        super().__init__()\n",
    "        self.pad_value = pad_value\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        self.net = net\n",
    "        self.max_seq_len = net.max_seq_len\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, start_tokens, seq_len, eos_token = None, temperature = 1., filter_logits_fn = top_k, filter_thres = 0.9, min_p_pow=2.0, min_p_ratio=0.02, **kwargs):\n",
    "        device = start_tokens.device\n",
    "        was_training = self.net.training\n",
    "        num_dims = len(start_tokens.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            start_tokens = start_tokens[None, :]\n",
    "\n",
    "        b, t = start_tokens.shape\n",
    "\n",
    "        self.net.eval()\n",
    "        out = start_tokens\n",
    "        mask = kwargs.pop('mask', None)\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            x = out[:, -self.max_seq_len:]\n",
    "            mask = mask[:, -self.max_seq_len:]\n",
    "\n",
    "            logits = self.net(x, mask=mask, **kwargs)[:, -1, :]\n",
    "\n",
    "            if filter_logits_fn in {top_k, top_p}:\n",
    "                filtered_logits = filter_logits_fn(logits, thres = filter_thres)\n",
    "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "\n",
    "            elif filter_logits_fn is top_a:\n",
    "                filtered_logits = filter_logits_fn(logits, min_p_pow = min_p_pow, min_p_ratio= min_p_ratio)\n",
    "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "\n",
    "            elif filter_logits_fn is entmax:\n",
    "                probs = entmax(logits / temperature, alpha = ENTMAX_ALPHA, dim=-1)\n",
    "\n",
    "            sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim=-1)\n",
    "            mask = F.pad(mask, (0, 1), value=True)\n",
    "\n",
    "            if exists(eos_token):\n",
    "                is_eos_tokens = (out == eos_token)\n",
    "\n",
    "                if is_eos_tokens.any(dim = -1).all():\n",
    "                    # mask out everything after the eos tokens\n",
    "                    shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))\n",
    "                    mask = shifted_is_eos_tokens.float().cumsum(dim = -1) >= 1\n",
    "                    out = out.masked_fill(mask, self.pad_value)\n",
    "                    break\n",
    "\n",
    "        out = out[:, t:]\n",
    "\n",
    "        if num_dims == 1:\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        self.net.train(was_training)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        xi = x[:, :-1]\n",
    "        xo = x[:, 1:]\n",
    "\n",
    "        # help auto-solve a frequent area of confusion around input masks in auto-regressive\n",
    "        # if user supplies a mask that is only off by one from the source sequence, resolve it for them\n",
    "        mask = kwargs.get('mask', None)\n",
    "        if mask is not None and mask.shape[1] == x.shape[1]:\n",
    "            mask = mask[:, :-1]\n",
    "            kwargs['mask'] = mask\n",
    "\n",
    "        out = self.net(xi, **kwargs)\n",
    "        loss = F.cross_entropy(out.transpose(1, 2), xo, ignore_index = self.ignore_index)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c66cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import torch\n",
    "# from torch import nn, einsum\n",
    "# import torch.nn.functional as F\n",
    "# from functools import partial\n",
    "# from inspect import isfunction\n",
    "# from collections import namedtuple\n",
    "\n",
    "# from einops import rearrange, repeat, reduce\n",
    "# from einops.layers.torch import Rearrange\n",
    "\n",
    "# from entmax import entmax15\n",
    "\n",
    "# constants\n",
    "\n",
    "DEFAULT_DIM_HEAD = 64\n",
    "\n",
    "Intermediates = namedtuple('Intermediates', [\n",
    "    'pre_softmax_attn',\n",
    "    'post_softmax_attn'\n",
    "])\n",
    "\n",
    "LayerIntermediates = namedtuple('Intermediates', [\n",
    "    'hiddens',\n",
    "    'attn_intermediates'\n",
    "])\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def cast_tuple(val, depth):\n",
    "    return val if isinstance(val, tuple) else (val,) * depth\n",
    "\n",
    "class always():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.val\n",
    "\n",
    "class not_equals():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        return x != self.val\n",
    "\n",
    "class equals():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        return x == self.val\n",
    "\n",
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max\n",
    "\n",
    "def l2norm(t):\n",
    "    return F.normalize(t, p = 2, dim = -1)\n",
    "\n",
    "def stable_softmax(t, dim = -1):\n",
    "    t = t - t.amax(dim = dim, keepdim = True).detach()\n",
    "    return F.softmax(t, dim = dim)\n",
    "\n",
    "# init helpers\n",
    "\n",
    "def init_zero_(layer):\n",
    "    nn.init.constant_(layer.weight, 0.)\n",
    "    if exists(layer.bias):\n",
    "        nn.init.constant_(layer.bias, 0.)\n",
    "\n",
    "# keyword argument helpers\n",
    "\n",
    "def pick_and_pop(keys, d):\n",
    "    values = list(map(lambda key: d.pop(key), keys))\n",
    "    return dict(zip(keys, values))\n",
    "\n",
    "def group_dict_by_key(cond, d):\n",
    "    return_val = [dict(),dict()]\n",
    "    for key in d.keys():\n",
    "        match = bool(cond(key))\n",
    "        ind = int(not match)\n",
    "        return_val[ind][key] = d[key]\n",
    "    return (*return_val,)\n",
    "\n",
    "def string_begins_with(prefix, str):\n",
    "    return str.startswith(prefix)\n",
    "\n",
    "def group_by_key_prefix(prefix, d):\n",
    "    return group_dict_by_key(partial(string_begins_with, prefix), d)\n",
    "\n",
    "def groupby_prefix_and_trim(prefix, d):\n",
    "    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n",
    "    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
    "    return kwargs_without_prefix, kwargs\n",
    "\n",
    "# activations\n",
    "\n",
    "class ReluSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.relu(x) ** 2\n",
    "\n",
    "# embedding\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed = False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "# positional embeddings\n",
    "\n",
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len, l2norm_embed = False):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5 if not l2norm_embed else 1.\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = torch.arange(x.shape[1], device = x.device)\n",
    "        pos_emb = self.emb(n)\n",
    "        pos_emb = rearrange(pos_emb, 'n d -> () n d')\n",
    "        pos_emb = pos_emb * self.scale\n",
    "        return l2norm(pos_emb) if self.l2norm_embed else pos_emb\n",
    "\n",
    "class FixedPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_dim = 1, offset = 0):\n",
    "        t = torch.arange(x.shape[seq_dim], device = x.device).type_as(self.inv_freq) + offset\n",
    "        sinusoid_inp = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return rearrange(emb, 'n d -> () n d')\n",
    "\n",
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, scale, causal = False, num_buckets = 32, max_distance = 128, heads = 8):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.causal = causal\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, causal = True, num_buckets = 32, max_distance = 128):\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "        if not causal:\n",
    "            num_buckets //= 2\n",
    "            ret += (n < 0).long() * num_buckets\n",
    "            n = torch.abs(n)\n",
    "        else:\n",
    "            n = torch.max(n, torch.zeros_like(n))\n",
    "\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).long()\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, qk_dots):\n",
    "        i, j, device = *qk_dots.shape[-2:], qk_dots.device\n",
    "        q_pos = torch.arange(i, dtype = torch.long, device = device)\n",
    "        k_pos = torch.arange(j, dtype = torch.long, device = device)\n",
    "        rel_pos = k_pos[None, :] - q_pos[:, None]\n",
    "        rp_bucket = self._relative_position_bucket(rel_pos, causal = self.causal, num_buckets = self.num_buckets, max_distance = self.max_distance)\n",
    "        values = self.relative_attention_bias(rp_bucket)\n",
    "        bias = rearrange(values, 'i j h -> () h i j')\n",
    "        return qk_dots + (bias * self.scale)\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, qk_dots):\n",
    "        n, device, dtype = qk_dots.shape[-1], qk_dots.device, qk_dots.dtype\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "\n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return qk_dots + bias\n",
    "\n",
    "class AlibiPositionalBias(nn.Module):\n",
    "    def __init__(self, heads, **kwargs):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        slopes = torch.Tensor(self._get_slopes(heads))\n",
    "        slopes = rearrange(slopes, 'h -> () h () ()')\n",
    "        self.register_buffer('slopes', slopes, persistent = False)\n",
    "        self.register_buffer('bias', None, persistent = False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_slopes(heads):\n",
    "        def get_slopes_power_of_2(n):\n",
    "            start = (2**(-2**-(math.log2(n)-3)))\n",
    "            ratio = start\n",
    "            return [start*ratio**i for i in range(n)]\n",
    "\n",
    "        if math.log2(heads).is_integer():\n",
    "            return get_slopes_power_of_2(heads)\n",
    "\n",
    "        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\n",
    "        return get_slopes_power_of_2(closest_power_of_2) + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:heads-closest_power_of_2]\n",
    "\n",
    "    def forward(self, qk_dots):\n",
    "        h, i, j, device = *qk_dots.shape[-3:], qk_dots.device\n",
    "\n",
    "        if exists(self.bias) and self.bias.shape[-1] >= j:\n",
    "            return qk_dots + self.bias[..., :j]\n",
    "\n",
    "        bias = torch.arange(j, device = device)\n",
    "        bias = rearrange(bias, 'j -> () () () j')\n",
    "        bias = bias * self.slopes\n",
    "\n",
    "        num_heads_unalibied = h - bias.shape[1]\n",
    "        bias = F.pad(bias, (0, 0, 0, 0, 0, num_heads_unalibied))\n",
    "\n",
    "        self.register_buffer('bias', bias, persistent = False)\n",
    "        return qk_dots + self.bias\n",
    "\n",
    "class LearnedAlibiPositionalBias(AlibiPositionalBias):\n",
    "    def __init__(self, heads, bidirectional = False):\n",
    "        super().__init__(heads)\n",
    "        log_slopes = torch.log(self.slopes)\n",
    "        self.learned_logslopes = nn.Parameter(log_slopes)\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        if self.bidirectional:\n",
    "            self.learned_logslopes_future = nn.Parameter(log_slopes)\n",
    "\n",
    "    def forward(self, qk_dots):\n",
    "        h, i, j, device = *qk_dots.shape[-3:], qk_dots.device\n",
    "\n",
    "        def get_slopes(param):\n",
    "            return F.pad(param.exp(), (0, 0, 0, 0, 0, h - param.shape[1]))\n",
    "\n",
    "        if exists(self.bias) and self.bias.shape[-1] >= j:\n",
    "            bias = self.bias[..., :i, :j]\n",
    "        else:\n",
    "            i_arange = torch.arange(i, device = device)\n",
    "            j_arange = torch.arange(j, device = device)\n",
    "            bias = rearrange(j_arange, 'j -> 1 1 1 j') - rearrange(i_arange, 'i -> 1 1 i 1')\n",
    "            self.register_buffer('bias', bias, persistent = False)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            past_slopes = get_slopes(self.learned_logslopes)\n",
    "            future_slopes = get_slopes(self.learned_logslopes_future)\n",
    "            bias = bias.abs()\n",
    "            bias = torch.tril(bias * past_slopes) + torch.triu(bias * future_slopes)\n",
    "        else:\n",
    "            slopes = get_slopes(self.learned_logslopes)\n",
    "            bias = bias * slopes\n",
    "\n",
    "        return qk_dots + bias\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, max_seq_len, device):\n",
    "        t = torch.arange(max_seq_len, device = device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return rearrange(emb, 'n d -> () () n d')\n",
    "\n",
    "def rotate_half(x):\n",
    "    x = rearrange(x, '... (j d) -> ... j d', j = 2)\n",
    "    x1, x2 = x.unbind(dim = -2)\n",
    "    return torch.cat((-x2, x1), dim = -1)\n",
    "\n",
    "def apply_rotary_pos_emb(t, freqs):\n",
    "    seq_len = t.shape[-2]\n",
    "    freqs = freqs[:, :, -seq_len:]\n",
    "    return (t * freqs.cos()) + (rotate_half(t) * freqs.sin())\n",
    "\n",
    "# norms\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, value, fn):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        out = self.fn(x, **kwargs)\n",
    "        scale_fn = lambda t: t * self.value\n",
    "\n",
    "        if not isinstance(out, tuple):\n",
    "            return scale_fn(out)\n",
    "\n",
    "        return (scale_fn(out[0]), *out[1:])\n",
    "\n",
    "class Rezero(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.g = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        out = self.fn(x, **kwargs)\n",
    "        rezero_fn = lambda t: t * self.g\n",
    "\n",
    "        if not isinstance(out, tuple):\n",
    "            return rezero_fn(out)\n",
    "\n",
    "        return (rezero_fn(out[0]), *out[1:])\n",
    "\n",
    "class ScaleNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.norm(x, dim = -1, keepdim = True) * self.scale\n",
    "        return x / norm.clamp(min = self.eps) * self.g\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.norm(x, dim = -1, keepdim = True) * self.scale\n",
    "        return x / norm.clamp(min = self.eps) * self.g\n",
    "\n",
    "# residual and residual gates\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, dim, scale_residual = False, scale_residual_constant = 1.):\n",
    "        super().__init__()\n",
    "        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n",
    "        self.scale_residual_constant = scale_residual_constant\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        if exists(self.residual_scale):\n",
    "            residual = residual * self.residual_scale\n",
    "\n",
    "        if self.scale_residual_constant != 1:\n",
    "            residual = residual * self.scale_residual_constant\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "class GRUGating(nn.Module):\n",
    "    def __init__(self, dim, scale_residual = False):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(dim, dim)\n",
    "        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        if exists(self.residual_scale):\n",
    "            residual = residual * self.residual_scale\n",
    "\n",
    "        gated_output = self.gru(\n",
    "            rearrange(x, 'b n d -> (b n) d'),\n",
    "            rearrange(residual, 'b n d -> (b n) d')\n",
    "        )\n",
    "\n",
    "        return gated_output.reshape_as(x)\n",
    "\n",
    "# token shifting\n",
    "\n",
    "def shift(t, amount, mask = None):\n",
    "    if amount == 0:\n",
    "        return t\n",
    "\n",
    "    if exists(mask):\n",
    "        t = t.masked_fill(~mask[..., None], 0.)\n",
    "\n",
    "    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n",
    "\n",
    "class ShiftTokens(nn.Module):\n",
    "    def __init__(self, shifts, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.shifts = tuple(shifts)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        mask = kwargs.get('mask', None)\n",
    "        shifts = self.shifts\n",
    "        segments = len(shifts)\n",
    "        feats_per_shift = x.shape[-1] // segments\n",
    "        splitted = x.split(feats_per_shift, dim = -1)\n",
    "        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n",
    "        segments_to_shift = list(map(lambda args: shift(*args, mask = mask), zip(segments_to_shift, shifts)))\n",
    "        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "# feedforward\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim = -1)\n",
    "        return x * self.act(gate)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_out = None,\n",
    "        mult = 4,\n",
    "        glu = False,\n",
    "        relu_squared = False,\n",
    "        post_act_ln = False,\n",
    "        dropout = 0.,\n",
    "        zero_init_output = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        dim_out = default(dim_out, dim)\n",
    "        activation = ReluSquared() if relu_squared else nn.GELU()\n",
    "\n",
    "        project_in = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim),\n",
    "            activation\n",
    "        ) if not glu else GLU(dim, inner_dim, activation)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            project_in,\n",
    "            nn.LayerNorm(inner_dim) if post_act_ln else nn.Identity(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim_out)\n",
    "        )\n",
    "\n",
    "        # init last linear layer to 0\n",
    "        if zero_init_output:\n",
    "            init_zero_(self.net[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# attention.\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head = DEFAULT_DIM_HEAD,\n",
    "        heads = 8,\n",
    "        causal = False,\n",
    "        talking_heads = False,\n",
    "        head_scale = False,\n",
    "        collab_heads = False,\n",
    "        collab_compression = .3,\n",
    "        sparse_topk = None,\n",
    "        use_entmax15 = False,\n",
    "        num_mem_kv = 0,\n",
    "        dropout = 0.,\n",
    "        on_attn = False,\n",
    "        gate_values = False,\n",
    "        zero_init_output = False,\n",
    "        max_attend_past = None,\n",
    "        qk_norm = False,\n",
    "        scale_init_value = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.max_attend_past = max_attend_past\n",
    "\n",
    "        qk_dim = v_dim = dim_head * heads\n",
    "\n",
    "        # collaborative heads\n",
    "        self.collab_heads = collab_heads\n",
    "        if self.collab_heads:\n",
    "            qk_dim = int(collab_compression * qk_dim)\n",
    "            self.collab_mixing = nn.Parameter(torch.randn(heads, qk_dim))\n",
    "\n",
    "        self.to_q = nn.Linear(dim, qk_dim, bias = False)\n",
    "        self.to_k = nn.Linear(dim, qk_dim, bias = False)\n",
    "        self.to_v = nn.Linear(dim, v_dim, bias = False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # add GLU gating for aggregated values, from alphafold2\n",
    "        self.to_v_gate = None\n",
    "        if gate_values:\n",
    "            self.to_v_gate = nn.Linear(dim, v_dim)\n",
    "            nn.init.constant_(self.to_v_gate.weight, 0)\n",
    "            nn.init.constant_(self.to_v_gate.bias, 1)\n",
    "\n",
    "        # cosine sim attention\n",
    "        self.qk_norm = qk_norm\n",
    "        if qk_norm:\n",
    "            scale_init_value = default(scale_init_value, -3) # if not provided, initialize as though it were sequence length of 1024\n",
    "            self.scale = nn.Parameter(torch.ones(1, heads, 1, 1) * scale_init_value)\n",
    "\n",
    "        # talking heads\n",
    "        self.talking_heads = talking_heads\n",
    "        if talking_heads:\n",
    "            self.pre_softmax_talking_heads = nn.Conv2d(heads, heads, 1, bias = False)\n",
    "            self.post_softmax_talking_heads = nn.Conv2d(heads, heads, 1, bias = False)\n",
    "\n",
    "        # head scaling\n",
    "        self.head_scale = head_scale\n",
    "        if head_scale:\n",
    "            self.head_scale_params = nn.Parameter(torch.ones(1, heads, 1, 1))\n",
    "\n",
    "        # explicit topk sparse attention\n",
    "        self.sparse_topk = sparse_topk\n",
    "\n",
    "        # entmax\n",
    "        self.attn_fn = entmax15 if use_entmax15 else stable_softmax\n",
    "\n",
    "        # add memory key / values\n",
    "        self.num_mem_kv = num_mem_kv\n",
    "        if num_mem_kv > 0:\n",
    "            self.mem_k = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n",
    "            self.mem_v = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n",
    "\n",
    "        # attention on attention\n",
    "        self.attn_on_attn = on_attn\n",
    "        self.to_out = nn.Sequential(nn.Linear(v_dim, dim * 2), nn.GLU()) if on_attn else nn.Linear(v_dim, dim)\n",
    "\n",
    "        # init output projection 0\n",
    "        if zero_init_output:\n",
    "            init_zero_(self.to_out)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        context = None,\n",
    "        mask = None,\n",
    "        context_mask = None,\n",
    "        attn_mask = None,\n",
    "        rel_pos = None,\n",
    "        sinusoidal_emb = None,\n",
    "        rotary_pos_emb = None,\n",
    "        prev_attn = None,\n",
    "        mem = None\n",
    "    ):\n",
    "        b, n, _, h, talking_heads, collab_heads, head_scale, scale, device, has_context = *x.shape, self.heads, self.talking_heads, self.collab_heads, self.head_scale, self.scale, x.device, exists(context)\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q_input = x\n",
    "        k_input = kv_input\n",
    "        v_input = kv_input\n",
    "\n",
    "        if exists(mem):\n",
    "            k_input = torch.cat((mem, k_input), dim = -2)\n",
    "            v_input = torch.cat((mem, v_input), dim = -2)\n",
    "\n",
    "        if exists(sinusoidal_emb):\n",
    "            # in shortformer, the query would start at a position offset depending on the past cached memory\n",
    "            offset = k_input.shape[-2] - q_input.shape[-2]\n",
    "            q_input = q_input + sinusoidal_emb(q_input, offset = offset)\n",
    "            k_input = k_input + sinusoidal_emb(k_input)\n",
    "\n",
    "        q = self.to_q(q_input)\n",
    "        k = self.to_k(k_input)\n",
    "        v = self.to_v(v_input)\n",
    "\n",
    "        if not collab_heads:\n",
    "            q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "        else:\n",
    "            q = einsum('b i d, h d -> b h i d', q, self.collab_mixing)\n",
    "            k = rearrange(k, 'b n d -> b () n d')\n",
    "            v = rearrange(v, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        if exists(rotary_pos_emb) and not has_context:\n",
    "            l = rotary_pos_emb.shape[-1]\n",
    "            (ql, qr), (kl, kr), (vl, vr) = map(lambda t: (t[..., :l], t[..., l:]), (q, k, v))\n",
    "            ql, kl, vl = map(lambda t: apply_rotary_pos_emb(t, rotary_pos_emb), (ql, kl, vl))\n",
    "            q, k, v = map(lambda t: torch.cat(t, dim = -1), ((ql, qr), (kl, kr), (vl, vr)))\n",
    "\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device = device).bool())\n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            input_mask = q_mask * k_mask\n",
    "\n",
    "        if self.num_mem_kv > 0:\n",
    "            mem_k, mem_v = map(lambda t: repeat(t, 'h n d -> b h n d', b = b), (self.mem_k, self.mem_v))\n",
    "            k = torch.cat((mem_k, k), dim = -2)\n",
    "            v = torch.cat((mem_v, v), dim = -2)\n",
    "            if exists(input_mask):\n",
    "                input_mask = F.pad(input_mask, (self.num_mem_kv, 0), value = True)\n",
    "\n",
    "        if collab_heads:\n",
    "            k = k.expand(-1, h, -1, -1)\n",
    "\n",
    "        if self.qk_norm:\n",
    "            q, k = map(l2norm, (q, k))\n",
    "            scale = 1 / (self.scale.exp().clamp(min = 1e-2))\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n",
    "        mask_value = max_neg_value(dots)\n",
    "\n",
    "        if exists(prev_attn):\n",
    "            dots = dots + prev_attn\n",
    "\n",
    "        pre_softmax_attn = dots.clone()\n",
    "\n",
    "        if talking_heads:\n",
    "            dots = self.pre_softmax_talking_heads(dots)\n",
    "\n",
    "        if exists(rel_pos):\n",
    "            dots = rel_pos(dots)\n",
    "\n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, mask_value)\n",
    "            del input_mask\n",
    "\n",
    "        if exists(attn_mask):\n",
    "            assert 2 <= attn_mask.ndim <= 4, 'attention mask must have greater than 2 dimensions but less than or equal to 4'\n",
    "            if attn_mask.ndim == 2:\n",
    "                attn_mask = rearrange(attn_mask, 'i j -> () () i j')\n",
    "            elif attn_mask.ndim == 3:\n",
    "                attn_mask = rearrange(attn_mask, 'h i j -> () h i j')\n",
    "            dots.masked_fill_(~attn_mask, mask_value)\n",
    "\n",
    "        if exists(self.max_attend_past):\n",
    "            i, j = dots.shape[-2:]\n",
    "            range_q = torch.arange(j - i, j, device = device)\n",
    "            range_k = torch.arange(j, device = device)\n",
    "            dist = rearrange(range_q, 'i -> () () i ()') - rearrange(range_k, 'j -> () () () j')\n",
    "            mask = dist > self.max_attend_past\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = dots.shape[-2:]\n",
    "            r = torch.arange(i, device = device)\n",
    "            mask = rearrange(r, 'i -> () () i ()') < rearrange(r, 'j -> () () () j')\n",
    "            mask = F.pad(mask, (j - i, 0), value = False)\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        if exists(self.sparse_topk) and self.sparse_topk < dots.shape[-1]:\n",
    "            top, _ = dots.topk(self.sparse_topk, dim = -1)\n",
    "            vk = top[..., -1].unsqueeze(-1).expand_as(dots)\n",
    "            mask = dots < vk\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        attn = self.attn_fn(dots, dim = -1)\n",
    "        post_softmax_attn = attn.clone()\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        if talking_heads:\n",
    "            attn = self.post_softmax_talking_heads(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "\n",
    "        if head_scale:\n",
    "            out = out * self.head_scale_params\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        if exists(self.to_v_gate):\n",
    "            gates = self.to_v_gate(x)\n",
    "            out = out * gates.sigmoid()\n",
    "\n",
    "        intermediates = Intermediates(\n",
    "            pre_softmax_attn = pre_softmax_attn,\n",
    "            post_softmax_attn = post_softmax_attn\n",
    "        )\n",
    "\n",
    "        return self.to_out(out), intermediates\n",
    "\n",
    "class AttentionLayers(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        causal = False,\n",
    "        cross_attend = False,\n",
    "        only_cross = False,\n",
    "        use_scalenorm = False,\n",
    "        use_rmsnorm = False,\n",
    "        use_rezero = False,\n",
    "        alibi_pos_bias = False,\n",
    "        alibi_num_heads = None,\n",
    "        alibi_learned = False,\n",
    "        rel_pos_bias = False,\n",
    "        rel_pos_num_buckets = 32,\n",
    "        rel_pos_max_distance = 128,\n",
    "        dynamic_pos_bias = False,\n",
    "        dynamic_pos_bias_log_distance = False,\n",
    "        dynamic_pos_bias_mlp_depth = 2,\n",
    "        dynamic_pos_bias_norm = False,\n",
    "        position_infused_attn = False,\n",
    "        rotary_pos_emb = False,\n",
    "        rotary_emb_dim = None,\n",
    "        custom_layers = None,\n",
    "        sandwich_coef = None,\n",
    "        par_ratio = None,\n",
    "        residual_attn = False,\n",
    "        cross_residual_attn = False,\n",
    "        macaron = False,\n",
    "        pre_norm = True,\n",
    "        gate_residual = False,\n",
    "        scale_residual = False,\n",
    "        scale_residual_constant = 1.,\n",
    "        shift_tokens = 0,\n",
    "        sandwich_norm = False,\n",
    "        use_qk_norm_attn = False,\n",
    "        qk_norm_attn_seq_len = None,\n",
    "        zero_init_branch_output = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ff_kwargs, kwargs = groupby_prefix_and_trim('ff_', kwargs)\n",
    "        attn_kwargs, kwargs = groupby_prefix_and_trim('attn_', kwargs)\n",
    "\n",
    "        dim_head = attn_kwargs.get('dim_head', DEFAULT_DIM_HEAD)\n",
    "\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        self.has_pos_emb = position_infused_attn or rel_pos_bias or rotary_pos_emb\n",
    "        self.pia_pos_emb = FixedPositionalEmbedding(dim) if position_infused_attn else None\n",
    "\n",
    "        rotary_emb_dim = max(default(rotary_emb_dim, dim_head // 2), 32)\n",
    "        self.rotary_pos_emb = RotaryEmbedding(rotary_emb_dim) if rotary_pos_emb else None\n",
    "\n",
    "        assert not (alibi_pos_bias and rel_pos_bias), 'you can only choose Alibi positional bias or T5 relative positional bias, not both'\n",
    "        assert rel_pos_num_buckets <= rel_pos_max_distance, 'number of relative position buckets must be less than the relative position max distance'\n",
    "\n",
    "        # relative positional bias\n",
    "\n",
    "        self.rel_pos = None\n",
    "        if rel_pos_bias:\n",
    "            self.rel_pos = RelativePositionBias(scale = dim_head ** 0.5, causal = causal, heads = heads, num_buckets = rel_pos_num_buckets, max_distance = rel_pos_max_distance)\n",
    "        elif dynamic_pos_bias:\n",
    "            self.rel_pos = DynamicPositionBias(dim = dim // 4, heads = heads, log_distance = dynamic_pos_bias_log_distance, depth = dynamic_pos_bias_mlp_depth, norm = dynamic_pos_bias_norm)\n",
    "        elif alibi_pos_bias:\n",
    "            alibi_num_heads = default(alibi_num_heads, heads)\n",
    "            assert alibi_num_heads <= heads, 'number of ALiBi heads must be less than the total number of heads'\n",
    "            alibi_pos_klass = LearnedAlibiPositionalBias if alibi_learned or not causal else AlibiPositionalBias\n",
    "            self.rel_pos = alibi_pos_klass(heads = alibi_num_heads, bidirectional = not causal)\n",
    "\n",
    "        assert not (not pre_norm and sandwich_norm), 'sandwich norm cannot be used when not using prenorm'\n",
    "        self.pre_norm = pre_norm\n",
    "        self.sandwich_norm = sandwich_norm\n",
    "\n",
    "        self.residual_attn = residual_attn\n",
    "        self.cross_residual_attn = cross_residual_attn\n",
    "        self.cross_attend = cross_attend\n",
    "\n",
    "        norm_class = ScaleNorm if use_scalenorm else nn.LayerNorm\n",
    "        norm_class = RMSNorm if use_rmsnorm else norm_class\n",
    "        norm_fn = partial(norm_class, dim)\n",
    "\n",
    "        norm_fn = nn.Identity if use_rezero else norm_fn\n",
    "        branch_fn = Rezero if use_rezero else None\n",
    "\n",
    "        if cross_attend and not only_cross:\n",
    "            default_block = ('a', 'c', 'f')\n",
    "        elif cross_attend and only_cross:\n",
    "            default_block = ('c', 'f')\n",
    "        else:\n",
    "            default_block = ('a', 'f')\n",
    "\n",
    "        if macaron:\n",
    "            default_block = ('f',) + default_block\n",
    "\n",
    "        # qk normalization\n",
    "\n",
    "        if use_qk_norm_attn:\n",
    "            attn_scale_init_value = -math.log(math.log2(qk_norm_attn_seq_len ** 2 - qk_norm_attn_seq_len)) if exists(qk_norm_attn_seq_len) else None\n",
    "            attn_kwargs = {**attn_kwargs, 'qk_norm': True, 'scale_init_value': attn_scale_init_value}\n",
    "\n",
    "        # zero init\n",
    "\n",
    "        if zero_init_branch_output:\n",
    "            attn_kwargs = {**attn_kwargs, 'zero_init_output':  True}\n",
    "            ff_kwargs = {**ff_kwargs, 'zero_init_output':  True}\n",
    "\n",
    "        # calculate layer block order\n",
    "\n",
    "        if exists(custom_layers):\n",
    "            layer_types = custom_layers\n",
    "        elif exists(par_ratio):\n",
    "            par_depth = depth * len(default_block)\n",
    "            assert 1 < par_ratio <= par_depth, 'par ratio out of range'\n",
    "            default_block = tuple(filter(not_equals('f'), default_block))\n",
    "            par_attn  = par_depth // par_ratio\n",
    "            depth_cut = par_depth * 2 // 3  # 2 / 3 attention layer cutoff suggested by PAR paper\n",
    "            par_width = (depth_cut + depth_cut // par_attn) // par_attn\n",
    "            assert len(default_block) <= par_width, 'default block is too large for par_ratio'\n",
    "            par_block = default_block + ('f',) * (par_width - len(default_block))\n",
    "            par_head = par_block * par_attn\n",
    "            layer_types = par_head + ('f',) * (par_depth - len(par_head))\n",
    "        elif exists(sandwich_coef):\n",
    "            assert sandwich_coef > 0 and sandwich_coef <= depth, 'sandwich coefficient should be less than the depth'\n",
    "            layer_types = ('a',) * sandwich_coef + default_block * (depth - sandwich_coef) + ('f',) * sandwich_coef\n",
    "        else:\n",
    "            layer_types = default_block * depth\n",
    "\n",
    "        self.layer_types = layer_types\n",
    "        self.num_attn_layers = len(list(filter(equals('a'), layer_types)))\n",
    "\n",
    "        # calculate token shifting\n",
    "\n",
    "        shift_tokens = cast_tuple(shift_tokens, len(layer_types))\n",
    "\n",
    "        # iterate and construct layers\n",
    "\n",
    "        for ind, (layer_type, layer_shift_tokens) in enumerate(zip(self.layer_types, shift_tokens)):\n",
    "            is_last_layer = ind == (len(self.layer_types) - 1)\n",
    "\n",
    "            if layer_type == 'a':\n",
    "                layer = Attention(dim, heads = heads, causal = causal, **attn_kwargs)\n",
    "            elif layer_type == 'c':\n",
    "                layer = Attention(dim, heads = heads, **attn_kwargs)\n",
    "            elif layer_type == 'f':\n",
    "                layer = FeedForward(dim, **ff_kwargs)\n",
    "                layer = layer if not macaron else Scale(0.5, layer)\n",
    "            else:\n",
    "                raise Exception(f'invalid layer type {layer_type}')\n",
    "\n",
    "            if layer_shift_tokens > 0:\n",
    "                shift_range_upper = layer_shift_tokens + 1\n",
    "                shift_range_lower = -layer_shift_tokens if not causal else 0\n",
    "                layer = ShiftTokens(range(shift_range_lower, shift_range_upper), layer)\n",
    "\n",
    "            if exists(branch_fn):\n",
    "                layer = branch_fn(layer)\n",
    "\n",
    "            residual_fn = GRUGating if gate_residual else Residual\n",
    "            residual = residual_fn(dim, scale_residual = scale_residual, scale_residual_constant = scale_residual_constant)\n",
    "\n",
    "            layer_uses_qk_norm = use_qk_norm_attn and layer_type in ('a', 'c')\n",
    "\n",
    "            pre_branch_norm = norm_fn() if pre_norm and not layer_uses_qk_norm else None\n",
    "            post_branch_norm = norm_fn() if sandwich_norm or layer_uses_qk_norm else None\n",
    "            post_main_norm = norm_fn() if not pre_norm and not is_last_layer else None\n",
    "\n",
    "            norms = nn.ModuleList([\n",
    "                pre_branch_norm,\n",
    "                post_branch_norm,\n",
    "                post_main_norm\n",
    "            ])\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                norms,\n",
    "                layer,\n",
    "                residual\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        context = None,\n",
    "        mask = None,\n",
    "        context_mask = None,\n",
    "        attn_mask = None,\n",
    "        mems = None,\n",
    "        return_hiddens = False\n",
    "    ):\n",
    "        assert not (self.cross_attend ^ exists(context)), 'context must be passed in if cross_attend is set to True'\n",
    "\n",
    "        hiddens = []\n",
    "        intermediates = []\n",
    "        prev_attn = None\n",
    "        prev_cross_attn = None\n",
    "\n",
    "        mems = mems.copy() if exists(mems) else [None] * self.num_attn_layers\n",
    "\n",
    "        rotary_pos_emb = None\n",
    "        if exists(self.rotary_pos_emb):\n",
    "            max_rotary_emb_length = max(list(map(lambda m: (m.shape[1] if exists(m) else 0) + x.shape[1], mems)))\n",
    "            rotary_pos_emb = self.rotary_pos_emb(max_rotary_emb_length, x.device)\n",
    "\n",
    "        for ind, (layer_type, (norm, block, residual_fn)) in enumerate(zip(self.layer_types, self.layers)):\n",
    "            is_last = ind == (len(self.layers) - 1)\n",
    "\n",
    "            if layer_type == 'a':\n",
    "                hiddens.append(x)\n",
    "                layer_mem = mems.pop(0) if mems else None\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            pre_branch_norm, post_branch_norm, post_main_norm = norm\n",
    "\n",
    "            if exists(pre_branch_norm):\n",
    "                x = pre_branch_norm(x)\n",
    "\n",
    "            if layer_type == 'a':\n",
    "                out, inter = block(x, mask = mask, attn_mask = attn_mask, sinusoidal_emb = self.pia_pos_emb, rel_pos = self.rel_pos, rotary_pos_emb = rotary_pos_emb, prev_attn = prev_attn, mem = layer_mem)\n",
    "            elif layer_type == 'c':\n",
    "                out, inter = block(x, context = context, mask = mask, context_mask = context_mask, prev_attn = prev_cross_attn)\n",
    "            elif layer_type == 'f':\n",
    "                out = block(x)\n",
    "\n",
    "            if exists(post_branch_norm):\n",
    "                out = post_branch_norm(out)\n",
    "\n",
    "            x = residual_fn(out, residual)\n",
    "\n",
    "            if layer_type in ('a', 'c'):\n",
    "                intermediates.append(inter)\n",
    "\n",
    "            if layer_type == 'a' and self.residual_attn:\n",
    "                prev_attn = inter.pre_softmax_attn\n",
    "            elif layer_type == 'c' and self.cross_residual_attn:\n",
    "                prev_cross_attn = inter.pre_softmax_attn\n",
    "\n",
    "            if exists(post_main_norm):\n",
    "                x = post_main_norm(x)\n",
    "\n",
    "        if return_hiddens:\n",
    "            intermediates = LayerIntermediates(\n",
    "                hiddens = hiddens,\n",
    "                attn_intermediates = intermediates\n",
    "            )\n",
    "\n",
    "            return x, intermediates\n",
    "\n",
    "        return x\n",
    "\n",
    "class Encoder(AttentionLayers):\n",
    "    def __init__(self, **kwargs):\n",
    "        assert 'causal' not in kwargs, 'cannot set causality on encoder'\n",
    "        super().__init__(causal = False, **kwargs)\n",
    "\n",
    "class Decoder(AttentionLayers):\n",
    "    def __init__(self, **kwargs):\n",
    "        assert 'causal' not in kwargs, 'cannot set causality on decoder'\n",
    "        super().__init__(causal = True, **kwargs)\n",
    "\n",
    "class CrossAttender(AttentionLayers):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(cross_attend = True, only_cross = True, **kwargs)\n",
    "\n",
    "class ViTransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        attn_layers,\n",
    "        num_classes = None,\n",
    "        dropout = 0.,\n",
    "        emb_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, Encoder), 'attention layers must be an Encoder'\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        dim = attn_layers.dim\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 3 * patch_size ** 2\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp_head = FeedForward(dim, dim_out = num_classes, dropout = dropout) if exists(num_classes) else None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        img,\n",
    "        return_embeddings = False\n",
    "    ):\n",
    "        p = self.patch_size\n",
    "\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "        x = self.patch_to_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.attn_layers(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if not exists(self.mlp_head) or return_embeddings:\n",
    "            return x\n",
    "\n",
    "        return self.mlp_head(x[:, 0])\n",
    "\n",
    "class TransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        num_tokens,\n",
    "        max_seq_len,\n",
    "        attn_layers,\n",
    "        emb_dim = None,\n",
    "        max_mem_len = 0.,\n",
    "        shift_mem_down = 0,\n",
    "        emb_dropout = 0.,\n",
    "        num_memory_tokens = None,\n",
    "        tie_embedding = False,\n",
    "        use_pos_emb = True,\n",
    "        l2norm_embed = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, AttentionLayers), 'attention layers must be one of Encoder or Decoder'\n",
    "\n",
    "        dim = attn_layers.dim\n",
    "        emb_dim = default(emb_dim, dim)\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_mem_len = max_mem_len\n",
    "        self.shift_mem_down = shift_mem_down\n",
    "\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.token_emb = TokenEmbedding(emb_dim, num_tokens, l2norm_embed = l2norm_embed)\n",
    "        self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len, l2norm_embed = l2norm_embed) if (use_pos_emb and not attn_layers.has_pos_emb) else always(0)\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.init_()\n",
    "\n",
    "        self.to_logits = nn.Linear(dim, num_tokens) if not tie_embedding else lambda t: t @ self.token_emb.weight.t()\n",
    "\n",
    "        # memory tokens (like [cls]) from Memory Transformers paper\n",
    "        num_memory_tokens = default(num_memory_tokens, 0)\n",
    "        self.num_memory_tokens = num_memory_tokens\n",
    "        if num_memory_tokens > 0:\n",
    "            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n",
    "\n",
    "    def init_(self):\n",
    "        if self.l2norm_embed:\n",
    "            nn.init.normal_(self.token_emb.emb.weight, std = 1e-5)\n",
    "            nn.init.normal_(self.pos_emb.emb.weight, std = 1e-5)\n",
    "            return\n",
    "\n",
    "        nn.init.kaiming_normal_(self.token_emb.emb.weight)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        return_embeddings = False,\n",
    "        mask = None,\n",
    "        return_mems = False,\n",
    "        return_attn = False,\n",
    "        mems = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        b, n, device, num_mem = *x.shape, x.device, self.num_memory_tokens\n",
    "\n",
    "        x = self.token_emb(x) + self.pos_emb(x)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        x = self.project_emb(x)\n",
    "\n",
    "        if num_mem > 0:\n",
    "            mem = repeat(self.memory_tokens, 'n d -> b n d', b = b)\n",
    "            x = torch.cat((mem, x), dim = 1)\n",
    "\n",
    "            # auto-handle masking after appending memory tokens\n",
    "            if exists(mask):\n",
    "                mask = F.pad(mask, (num_mem, 0), value = True)\n",
    "\n",
    "        if self.shift_mem_down and exists(mems):\n",
    "            mems_l, mems_r = mems[:self.shift_mem_down], mems[self.shift_mem_down:]\n",
    "            mems = [*mems_r, *mems_l]\n",
    "\n",
    "        x, intermediates = self.attn_layers(x, mask = mask, mems = mems, return_hiddens = True, **kwargs)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        mem, x = x[:, :num_mem], x[:, num_mem:]\n",
    "\n",
    "        out = self.to_logits(x) if not return_embeddings else x\n",
    "\n",
    "        if return_mems:\n",
    "            hiddens = intermediates.hiddens\n",
    "            new_mems = list(map(lambda pair: torch.cat(pair, dim = -2), zip(mems, hiddens))) if exists(mems) else hiddens\n",
    "            new_mems = list(map(lambda t: t[..., -self.max_mem_len:, :].detach(), new_mems))\n",
    "            return out, new_mems\n",
    "\n",
    "        if return_attn:\n",
    "            attn_maps = list(map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates))\n",
    "            return out, attn_maps\n",
    "\n",
    "        return out\n",
    "\n",
    "class ContinuousTransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        max_seq_len,\n",
    "        attn_layers,\n",
    "        dim_in = None,\n",
    "        dim_out = None,\n",
    "        emb_dim = None,\n",
    "        emb_dropout = 0.,\n",
    "        use_pos_emb = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, AttentionLayers), 'attention layers must be one of Encoder or Decoder'\n",
    "\n",
    "        dim = attn_layers.dim\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.pos_emb = AbsolutePositionalEmbedding(dim, max_seq_len) if (use_pos_emb and not attn_layers.has_pos_emb) else always(0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.project_in = nn.Linear(dim_in, dim) if exists(dim_in) else nn.Identity()\n",
    "\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.project_out = nn.Linear(dim, dim_out) if exists(dim_out) else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        return_embeddings = False,\n",
    "        mask = None,\n",
    "        return_attn = False,\n",
    "        mems = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        b, n, _, device = *x.shape, x.device\n",
    "\n",
    "        x = self.project_in(x)\n",
    "        x = x + self.pos_emb(x)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        x, intermediates = self.attn_layers(x, mask = mask, mems = mems, return_hiddens = True, **kwargs)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        out = self.project_out(x) if not return_embeddings else x\n",
    "\n",
    "        if return_attn:\n",
    "            attn_maps = list(map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates))\n",
    "            return out, attn_maps\n",
    "\n",
    "        return out\n",
    "\n",
    "class XTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        tie_token_emb = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        enc_kwargs, kwargs = groupby_prefix_and_trim('enc_', kwargs)\n",
    "        dec_kwargs, kwargs = groupby_prefix_and_trim('dec_', kwargs)\n",
    "\n",
    "        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs, 'dimension of either encoder or decoder must be set with `dim` keyword'\n",
    "        enc_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], enc_kwargs)\n",
    "        enc_transformer_kwargs['emb_dropout'] = enc_kwargs.pop('emb_dropout', 0)\n",
    "        enc_transformer_kwargs['num_memory_tokens'] = enc_kwargs.pop('num_memory_tokens', None)\n",
    "\n",
    "        dec_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], dec_kwargs)\n",
    "        dec_transformer_kwargs['emb_dropout'] = dec_kwargs.pop('emb_dropout', 0)\n",
    "\n",
    "        self.encoder = TransformerWrapper(\n",
    "            **enc_transformer_kwargs,\n",
    "            attn_layers = Encoder(dim = dim, **enc_kwargs)\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerWrapper(\n",
    "            **dec_transformer_kwargs,\n",
    "            attn_layers = Decoder(dim = dim, cross_attend = True, **dec_kwargs)\n",
    "        )\n",
    "\n",
    "        if tie_token_emb:\n",
    "            self.decoder.token_emb = self.encoder.token_emb\n",
    "\n",
    "        self.decoder = AutoregressiveWrapper(self.decoder)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, seq_in, seq_out_start, seq_len, src_mask = None, src_attn_mask = None, **kwargs):\n",
    "        encodings = self.encoder(seq_in, mask = src_mask, attn_mask = src_attn_mask, return_embeddings = True)\n",
    "        return self.decoder.generate(seq_out_start, seq_len, context = encodings, context_mask = src_mask, **kwargs)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask = None, tgt_mask = None, src_attn_mask = None):\n",
    "        enc = self.encoder(src, mask = src_mask, attn_mask = src_attn_mask, return_embeddings = True)\n",
    "        out = self.decoder(tgt, context = enc, mask = tgt_mask, context_mask = src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66930596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669b9109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x64 at 0x7FF1478A69D0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# img = Image.open(\"./dataset/sample/1000a29807.png\")\n",
    "img = Image.open(\"./dataset/sample/10024a5ccf.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f12afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from x_transformers import *\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "# from x_transformers.autoregressive_wrapper import AutoregressiveWrapper, top_k, top_p, entmax, ENTMAX_ALPHA\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from timm.models.vision_transformer_hybrid import HybridEmbed\n",
    "from timm.models.resnetv2 import ResNetV2\n",
    "from timm.models.layers import StdConv2dSame\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class CustomARWrapper(AutoregressiveWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomARWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, start_tokens, seq_len, eos_token=None, temperature=1., filter_logits_fn=top_k, filter_thres=0.9, **kwargs):\n",
    "        device = start_tokens.device\n",
    "        was_training = self.net.training\n",
    "        num_dims = len(start_tokens.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            start_tokens = start_tokens[None, :]\n",
    "\n",
    "        b, t = start_tokens.shape\n",
    "\n",
    "        self.net.eval()\n",
    "        out = start_tokens\n",
    "        mask = kwargs.pop('mask', None)\n",
    "        if mask is None:\n",
    "            mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            x = out[:, -self.max_seq_len:]\n",
    "            mask = mask[:, -self.max_seq_len:]\n",
    "            # print('arw:',out.shape)\n",
    "            logits = self.net(x, mask=mask, **kwargs)[:, -1, :]\n",
    "\n",
    "            if filter_logits_fn in {top_k, top_p}:\n",
    "                filtered_logits = filter_logits_fn(logits, thres=filter_thres)\n",
    "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "\n",
    "            elif filter_logits_fn is entmax:\n",
    "                probs = entmax(logits / temperature, alpha=ENTMAX_ALPHA, dim=-1)\n",
    "\n",
    "            sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim=-1)\n",
    "            mask = F.pad(mask, (0, 1), value=True)\n",
    "\n",
    "            if eos_token is not None and (torch.cumsum(out == eos_token, 1)[:, -1] >= 1).all():\n",
    "                break\n",
    "\n",
    "        out = out[:, t:]\n",
    "\n",
    "        if num_dims == 1:\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        self.net.train(was_training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CustomVisionTransformer(VisionTransformer):\n",
    "    def __init__(self, img_size=224, patch_size=16, *args, **kwargs):\n",
    "        super(CustomVisionTransformer, self).__init__(img_size=img_size, patch_size=patch_size, *args, **kwargs)\n",
    "        self.height, self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, c, h, w = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        h, w = h//self.patch_size, w//self.patch_size\n",
    "        pos_emb_ind = repeat(torch.arange(h)*(self.width//self.patch_size-w), 'h -> (h w)', w=w)+torch.arange(h*w)\n",
    "        pos_emb_ind = torch.cat((torch.zeros(1), pos_emb_ind+1), dim=0).long()\n",
    "        x += self.pos_embed[:, pos_emb_ind]\n",
    "        #x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder: CustomVisionTransformer, decoder: CustomARWrapper, args, temp: float = .333):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.bos_token = args.bos_token\n",
    "        self.eos_token = args.eos_token\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.temperature = temp\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        device = x.device\n",
    "        encoded = self.encoder(x.to(device))\n",
    "        dec = self.decoder.generate(torch.LongTensor([self.bos_token]*len(x))[:, None].to(device), self.max_seq_len,\n",
    "                                    eos_token=self.eos_token, context=encoded, temperature=self.temperature)\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599f4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "last_pic = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef1364b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lap14784/Downloads/LaTeX_OCR'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/lap14784/Downloads/LaTeX_OCR\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e019704",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = None\n",
    "if arguments is None:\n",
    "    arguments = Munch({'config': 'settings/config.yaml', 'checkpoint': 'checkpoints/weights.pth', 'no_cuda': True, 'no_resize': False})\n",
    "logging.getLogger().setLevel(logging.FATAL)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "with open(arguments.config, 'r') as f:\n",
    "    params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "args = parse_args(Munch(params))\n",
    "args.update(**vars(arguments))\n",
    "args.wandb = False\n",
    "# args.device = \"cpu\"\n",
    "args.device = 'cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54761cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ResNetV2(\n",
    "    layers=args.backbone_layers, num_classes=0, global_pool='', in_chans=args.channels,\n",
    "    preact=False, stem_type='same', conv_layer=StdConv2dSame)\n",
    "min_patch_size = 2**(len(args.backbone_layers)+1)\n",
    "\n",
    "def embed_layer(**x):\n",
    "    ps = x.pop('patch_size', min_patch_size)\n",
    "    assert ps % min_patch_size == 0 and ps >= min_patch_size, 'patch_size needs to be multiple of %i with current backbone configuration' % min_patch_size\n",
    "    return HybridEmbed(**x, patch_size=ps//min_patch_size, backbone=backbone)\n",
    "\n",
    "encoder = CustomVisionTransformer(img_size=(args.max_height, args.max_width),\n",
    "                                  patch_size=args.patch_size,\n",
    "                                  in_chans=args.channels,\n",
    "                                  num_classes=0,\n",
    "                                  embed_dim=args.dim,\n",
    "                                  depth=args.encoder_depth,\n",
    "                                  num_heads=args.heads,\n",
    "                                  embed_layer=embed_layer\n",
    "                                  ).to(args.device)\n",
    "\n",
    "decoder = CustomARWrapper(\n",
    "    TransformerWrapper(\n",
    "        num_tokens=args.num_tokens,\n",
    "        max_seq_len=args.max_seq_len,\n",
    "        attn_layers=Decoder(\n",
    "            dim=args.dim,\n",
    "            depth=args.num_layers,\n",
    "            heads=args.heads,\n",
    "            **args.decoder_args\n",
    "        )),\n",
    "    pad_value=args.pad_token\n",
    ").to(args.device)\n",
    "if 'wandb' in args and args.wandb:\n",
    "    import wandb\n",
    "    wandb.watch((encoder, decoder.net.attn_layers))\n",
    "model = Model(encoder, decoder, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "824b45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_size(img, max_dimensions=None, min_dimensions=None):\n",
    "    if max_dimensions is not None:\n",
    "        ratios = [a/b for a, b in zip(img.size, max_dimensions)]\n",
    "        if any([r > 1 for r in ratios]):\n",
    "            size = np.array(img.size)//max(ratios)\n",
    "            img = img.resize(size.astype(int), Image.BILINEAR)\n",
    "    if min_dimensions is not None:\n",
    "        if any([s < min_dimensions[i] for i, s in enumerate(img.size)]):\n",
    "            padded_im = Image.new('L', min_dimensions, 255)\n",
    "            padded_im.paste(img, img.getbbox())\n",
    "            img = padded_im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c39178fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments = Munch({'epoch': 0, 'backbone_layers': [2, 3, 7], 'betas': [0.9, 0.999], 'batchsize': 10, 'bos_token': 1, 'channels': 1, 'data': 'dataset/data/train.pkl', 'debug': False, 'decoder_args': {'attn_on_attn': True, 'cross_attend': True, 'ff_glu': True, 'rel_pos_bias': False, 'use_scalenorm': False}, 'dim': 256, 'encoder_depth': 4, 'eos_token': 2, 'epochs': 10, 'gamma': 0.9995, 'heads': 8, 'id': None, 'load_chkpt': None, 'lr': 0.001, 'lr_step': 30, 'max_height': 192, 'max_seq_len': 512, 'max_width': 672, 'min_height': 32, 'min_width': 32, 'model_path': 'checkpoints', 'name': 'pix2tex', 'num_layers': 4, 'num_tokens': 8000, 'optimizer': 'Adam', 'output_path': 'outputs', 'pad': False, 'pad_token': 0, 'patch_size': 16, 'sample_freq': 3000, 'save_freq': 5, 'scheduler': 'StepLR', 'seed': 42, 'temperature': 0.2, 'test_samples': 5, 'testbatchsize': 20, 'tokenizer': 'dataset/tokenizer.json', 'valbatches': 100, 'valdata': 'dataset/data/val.pkl', 'wandb': False, 'device': 'cpu', 'max_dimensions': [672, 192], 'min_dimensions': [32, 32], 'out_path': 'checkpoints/pix2tex', 'config': 'settings/config.yaml', 'checkpoint': 'checkpoints/weights.pth', 'no_cuda': False, 'no_resize': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf1ac292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n",
    "\n",
    "if 'image_resizer.pth' in os.listdir(os.path.dirname(args.checkpoint)) and not arguments.no_resize:\n",
    "    image_resizer = ResNetV2(layers=[2, 3, 3], num_classes=max(args.max_dimensions)//32, global_pool='avg', in_chans=1, drop_rate=.05,\n",
    "                             preact=True, stem_type='same', conv_layer=StdConv2dSame).to(args.device)\n",
    "    image_resizer.load_state_dict(torch.load(os.path.join(os.path.dirname(args.checkpoint), 'image_resizer.pth'), map_location=args.device))\n",
    "    image_resizer.eval()\n",
    "else:\n",
    "    image_resizer = None\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=args.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9b85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47467447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 160])\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = model.encoder, model.decoder\n",
    "if type(img) is bool:\n",
    "    img = None\n",
    "if img is None:\n",
    "    if last_pic is None:\n",
    "        print('Provide an image.')\n",
    "    else:\n",
    "        img = last_pic.copy()\n",
    "else:\n",
    "    last_pic = img.copy()\n",
    "img = minmax_size(pad(img), args.max_dimensions, args.min_dimensions)\n",
    "if image_resizer is not None and not args.no_resize:\n",
    "    with torch.no_grad():\n",
    "        input_image = img.convert('RGB').copy()\n",
    "        r, w, h = 1, input_image.size[0], input_image.size[1]\n",
    "        for _ in range(10):\n",
    "            h = int(h * r)  # height to resize\n",
    "            img = pad(minmax_size(input_image.resize((w, h), Image.BILINEAR if r > 1 else Image.LANCZOS), args.max_dimensions, args.min_dimensions))\n",
    "            t = test_transform(image=np.array(img.convert('RGB')))['image'][:1].unsqueeze(0)\n",
    "            w = (image_resizer(t.to(args.device)).argmax(-1).item()+1)*32\n",
    "            logging.info(r, img.size, (w, int(input_image.size[1]*r)))\n",
    "            if (w == img.size[0]):\n",
    "                break\n",
    "            r = w/img.size[0]\n",
    "else:\n",
    "    img = np.array(pad(img).convert('RGB'))\n",
    "    t = test_transform(image=img)['image'][:1].unsqueeze(0)\n",
    "im = t.to(args.device)\n",
    "print(np.shape(im))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    device = args.device\n",
    "    encoded = encoder(im.to(device))\n",
    "    dec = decoder.generate(torch.LongTensor([args.bos_token])[:, None].to(device), args.max_seq_len,\n",
    "                           eos_token=args.eos_token, context=encoded.detach(), temperature=args.get('temperature', .25))\n",
    "    pred = post_process(token2str(dec, tokenizer)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c4c1bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V_{2}(x)=-{\\\\frac{1}{2}}x^{2}-{\\\\frac{\\\\mu^{2}}{2x^{2}}},'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d5b89b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <html>\n",
       "        <head><script id=\"MathJax-script\" src=\"qrc:MathJax.js\"></script>\n",
       "        <script>\n",
       "        MathJax.Hub.Config({messageStyle: 'none',tex2jax: {preview: 'none'}});\n",
       "        MathJax.Hub.Queue(\n",
       "            function () {\n",
       "                document.getElementById(\"equation\").style.visibility = \"\";\n",
       "            }\n",
       "            );\n",
       "        </script>\n",
       "        </head> \n",
       "        <body>\n",
       "        <div id=\"equation\" style=\"font-size:1em; visibility:hidden\">$$V_{2}(x)=-{\\frac{1}{2}}x^{2}-{\\frac{\\mu^{2}}{2x^{2}}},$$</div>\n",
       "        </body>\n",
       "        </html>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pageSource = \"\"\"\n",
    "        <html>\n",
    "        <head><script id=\"MathJax-script\" src=\"qrc:MathJax.js\"></script>\n",
    "        <script>\n",
    "        MathJax.Hub.Config({messageStyle: 'none',tex2jax: {preview: 'none'}});\n",
    "        MathJax.Hub.Queue(\n",
    "            function () {\n",
    "                document.getElementById(\"equation\").style.visibility = \"\";\n",
    "            }\n",
    "            );\n",
    "        </script>\n",
    "        </head> \"\"\" + \"\"\"\n",
    "        <body>\n",
    "        <div id=\"equation\" style=\"font-size:1em; visibility:hidden\">$${equation}$$</div>\n",
    "        </body>\n",
    "        </html>\n",
    "            \"\"\".format(equation=pred)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(pageSource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262edc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584dd2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303e402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825bf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613fcca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f05f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
