{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas.io.clipboard as clipboard\n",
    "from PIL import ImageGrab\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from munch import Munch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from timm.models.resnetv2 import ResNetV2\n",
    "from timm.models.layers import StdConv2dSame\n",
    "from dataset.dataset import test_transform\n",
    "\n",
    "from dataset.latex2png import tex2pil\n",
    "from models import get_model\n",
    "from utils import *\n",
    "\n",
    "last_pic = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lap14784/Downloads/LaTeX_OCR'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/lap14784/Downloads/LaTeX_OCR\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from x_transformers import *\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "from x_transformers.autoregressive_wrapper import AutoregressiveWrapper, top_k, top_p, entmax, ENTMAX_ALPHA\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from timm.models.vision_transformer_hybrid import HybridEmbed\n",
    "from timm.models.resnetv2 import ResNetV2\n",
    "from timm.models.layers import StdConv2dSame\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class CustomARWrapper(AutoregressiveWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomARWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, start_tokens, seq_len, eos_token=None, temperature=1., filter_logits_fn=top_k, filter_thres=0.9, **kwargs):\n",
    "        device = start_tokens.device\n",
    "        was_training = self.net.training\n",
    "        num_dims = len(start_tokens.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            start_tokens = start_tokens[None, :]\n",
    "\n",
    "        b, t = start_tokens.shape\n",
    "\n",
    "        self.net.eval()\n",
    "        out = start_tokens\n",
    "        mask = kwargs.pop('mask', None)\n",
    "        if mask is None:\n",
    "            mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            x = out[:, -self.max_seq_len:]\n",
    "            mask = mask[:, -self.max_seq_len:]\n",
    "            # print('arw:',out.shape)\n",
    "            logits = self.net(x, mask=mask, **kwargs)[:, -1, :]\n",
    "\n",
    "            if filter_logits_fn in {top_k, top_p}:\n",
    "                filtered_logits = filter_logits_fn(logits, thres=filter_thres)\n",
    "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "\n",
    "            elif filter_logits_fn is entmax:\n",
    "                probs = entmax(logits / temperature, alpha=ENTMAX_ALPHA, dim=-1)\n",
    "\n",
    "            sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim=-1)\n",
    "            mask = F.pad(mask, (0, 1), value=True)\n",
    "\n",
    "            if eos_token is not None and (torch.cumsum(out == eos_token, 1)[:, -1] >= 1).all():\n",
    "                break\n",
    "\n",
    "        out = out[:, t:]\n",
    "\n",
    "        if num_dims == 1:\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        self.net.train(was_training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CustomVisionTransformer(VisionTransformer):\n",
    "    def __init__(self, img_size=224, patch_size=16, *args, **kwargs):\n",
    "        super(CustomVisionTransformer, self).__init__(img_size=img_size, patch_size=patch_size, *args, **kwargs)\n",
    "        self.height, self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        print(np.shape(x))\n",
    "        B, c, h, w = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        h, w = h//self.patch_size, w//self.patch_size\n",
    "        pos_emb_ind = repeat(torch.arange(h)*(self.width//self.patch_size-w), 'h -> (h w)', w=w)+torch.arange(h*w)\n",
    "        pos_emb_ind = torch.cat((torch.zeros(1), pos_emb_ind+1), dim=0).long()\n",
    "        x += self.pos_embed[:, pos_emb_ind]\n",
    "        #x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder: CustomVisionTransformer, decoder: CustomARWrapper, args, temp: float = .333):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.bos_token = args.bos_token\n",
    "        self.eos_token = args.eos_token\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.temperature = temp\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        print(\"forward\", x)\n",
    "        device = x.device\n",
    "        encoded = self.encoder(x.to(device))\n",
    "        dec = self.decoder.generate(torch.LongTensor([self.bos_token]*len(x))[:, None].to(device), self.max_seq_len,\n",
    "                                    eos_token=self.eos_token, context=encoded, temperature=self.temperature)\n",
    "        return dec\n",
    "\n",
    "\n",
    "def get_model(args, training=False):\n",
    "    backbone = ResNetV2(\n",
    "        layers=args.backbone_layers, num_classes=0, global_pool='', in_chans=args.channels,\n",
    "        preact=False, stem_type='same', conv_layer=StdConv2dSame)\n",
    "    min_patch_size = 2**(len(args.backbone_layers)+1)\n",
    "\n",
    "    def embed_layer(**x):\n",
    "        ps = x.pop('patch_size', min_patch_size)\n",
    "        assert ps % min_patch_size == 0 and ps >= min_patch_size, 'patch_size needs to be multiple of %i with current backbone configuration' % min_patch_size\n",
    "        return HybridEmbed(**x, patch_size=ps//min_patch_size, backbone=backbone)\n",
    "\n",
    "    encoder = CustomVisionTransformer(img_size=(args.max_height, args.max_width),\n",
    "                                      patch_size=args.patch_size,\n",
    "                                      in_chans=args.channels,\n",
    "                                      num_classes=0,\n",
    "                                      embed_dim=args.dim,\n",
    "                                      depth=args.encoder_depth,\n",
    "                                      num_heads=args.heads,\n",
    "                                      embed_layer=embed_layer\n",
    "                                      ).to(args.device)\n",
    "\n",
    "    decoder = CustomARWrapper(\n",
    "        TransformerWrapper(\n",
    "            num_tokens=args.num_tokens,\n",
    "            max_seq_len=args.max_seq_len,\n",
    "            attn_layers=Decoder(\n",
    "                dim=args.dim,\n",
    "                depth=args.num_layers,\n",
    "                heads=args.heads,\n",
    "                **args.decoder_args\n",
    "            )),\n",
    "        pad_value=args.pad_token\n",
    "    ).to(args.device)\n",
    "\n",
    "    model = Model(encoder, decoder, args)\n",
    "#     if training:\n",
    "#         # check if largest batch can be handled by system\n",
    "#         im = torch.empty(args.batchsize, args.channels, args.max_height, args.min_height, device=args.device).float()\n",
    "#         seq = torch.randint(0, args.num_tokens, (args.batchsize, args.max_seq_len), device=args.device).long()\n",
    "#         decoder(seq, context=encoder(im)).sum().backward()\n",
    "#         model.zero_grad()\n",
    "#         torch.cuda.empty_cache() \n",
    "#         del im, seq\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from dataset.dataset import test_transform\n",
    "import cv2\n",
    "import pandas.io.clipboard as clipboard\n",
    "from PIL import ImageGrab\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from munch import Munch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from timm.models.resnetv2 import ResNetV2\n",
    "from timm.models.layers import StdConv2dSame\n",
    "\n",
    "from dataset.latex2png import tex2pil\n",
    "from models import get_model\n",
    "from utils import *\n",
    "last_pic = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward tensor([[[[[0.6461, 0.9796, 0.5770,  ..., 0.0884, 0.3716, 0.7028],\n",
      "           [0.8204, 0.4483, 0.2467,  ..., 0.2225, 0.6587, 0.7426],\n",
      "           [0.6052, 0.4622, 0.7299,  ..., 0.4954, 0.2070, 0.9071],\n",
      "           ...,\n",
      "           [0.0564, 0.4330, 0.6895,  ..., 0.0263, 0.9023, 0.3936],\n",
      "           [0.2982, 0.2558, 0.8864,  ..., 0.4704, 0.9282, 0.7937],\n",
      "           [0.4343, 0.1507, 0.2430,  ..., 0.8863, 0.6885, 0.9142]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.6531, 0.7680, 0.2409,  ..., 0.6531, 0.3143, 0.2850],\n",
      "           [0.4735, 0.0383, 0.5654,  ..., 0.0679, 0.4456, 0.6908],\n",
      "           [0.7435, 0.8656, 0.9616,  ..., 0.0239, 0.5107, 0.7927],\n",
      "           ...,\n",
      "           [0.8872, 0.7160, 0.3235,  ..., 0.5862, 0.6248, 0.2348],\n",
      "           [0.8961, 0.3489, 0.6081,  ..., 0.1254, 0.4765, 0.9102],\n",
      "           [0.1613, 0.3081, 0.9532,  ..., 0.6353, 0.5779, 0.8079]]]]])\n",
      "torch.Size([2, 1, 1, 64, 352])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_127699/1788777313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m352\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# summary(model, (1, 1, 64, 352))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_127699/703272342.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"forward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         dec = self.decoder.generate(torch.LongTensor([self.bos_token]*len(x))[:, None].to(device), self.max_seq_len,\n\u001b[1;32m    109\u001b[0m                                     eos_token=self.eos_token, context=encoded, temperature=self.temperature)\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/colab/lib/python3.7/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dist\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# x must be a tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_127699/703272342.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# if arguments is None:\n",
    "# arguments = Munch({'config': 'settings/config.yaml', 'checkpoint': 'checkpoints/weights.pth', 'no_cuda': True, 'no_resize': False})\n",
    "arguments = Munch({'epoch': 0, 'backbone_layers': [2, 3, 7], 'betas': [0.9, 0.999], 'batchsize': 10, 'bos_token': 1, 'channels': 1, 'data': 'dataset/data/train.pkl', 'debug': False, 'decoder_args': {'attn_on_attn': True, 'cross_attend': True, 'ff_glu': True, 'rel_pos_bias': False, 'use_scalenorm': False}, 'dim': 256, 'encoder_depth': 4, 'eos_token': 2, 'epochs': 10, 'gamma': 0.9995, 'heads': 8, 'id': None, 'load_chkpt': None, 'lr': 0.001, 'lr_step': 30, 'max_height': 192, 'max_seq_len': 512, 'max_width': 672, 'min_height': 32, 'min_width': 32, 'model_path': 'checkpoints', 'name': 'pix2tex', 'num_layers': 4, 'num_tokens': 8000, 'optimizer': 'Adam', 'output_path': 'outputs', 'pad': False, 'pad_token': 0, 'patch_size': 16, 'sample_freq': 3000, 'save_freq': 5, 'scheduler': 'StepLR', 'seed': 42, 'temperature': 0.2, 'test_samples': 5, 'testbatchsize': 20, 'tokenizer': 'dataset/tokenizer.json', 'valbatches': 100, 'valdata': 'dataset/data/val.pkl', 'wandb': False, 'device': 'cpu', 'max_dimensions': [672, 192], 'min_dimensions': [32, 32], 'out_path': 'checkpoints/pix2tex', 'config': 'settings/config.yaml', 'checkpoint': 'checkpoints/weights.pth', 'no_cuda': False, 'no_resize': False})\n",
    "# logging.getLogger().setLevel(logging.FATAL)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "with open(arguments.config, 'r') as f:\n",
    "    params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "args = parse_args(Munch(params))\n",
    "args.update(**vars(arguments))\n",
    "# args.device = \"cpu\"\n",
    "args.device = 'cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu'\n",
    "\n",
    "model = get_model(args)\n",
    "# summary(model, (1, 1, 64, 352))\n",
    "# model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n",
    "# summary(model, (1, 1, 64, 352))\n",
    "\n",
    "# if 'image_resizer.pth' in os.listdir(os.path.dirname(args.checkpoint)) and not arguments.no_resize:\n",
    "#     image_resizer = ResNetV2(layers=[2, 3, 3], num_classes=max(args.max_dimensions)//32, global_pool='avg', in_chans=1, drop_rate=.05,\n",
    "#                              preact=True, stem_type='same', conv_layer=StdConv2dSame).to(args.device)\n",
    "#     image_resizer.load_state_dict(torch.load(os.path.join(os.path.dirname(args.checkpoint), 'image_resizer.pth'), map_location=args.device))\n",
    "#     image_resizer.eval()\n",
    "# else:\n",
    "#     image_resizer = None\n",
    "# tokenizer = PreTrainedTokenizerFast(tokenizer_file=args.tokenizer)\n",
    "# return args, model, image_resizer, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"./dataset/sample/1000a29807.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 352])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size([1, 1, 64, 352])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = model.encoder, model.decoder\n",
    "if type(img) is bool:\n",
    "    img = None\n",
    "if img is None:\n",
    "    if last_pic is None:\n",
    "        print('Provide an image.')\n",
    "    else:\n",
    "        img = last_pic.copy()\n",
    "else:\n",
    "    last_pic = img.copy()\n",
    "img = minmax_size(pad(img), args.max_dimensions, args.min_dimensions)\n",
    "if image_resizer is not None and not args.no_resize:\n",
    "    with torch.no_grad():\n",
    "        input_image = img.convert('RGB').copy()\n",
    "        r, w, h = 1, input_image.size[0], input_image.size[1]\n",
    "        for _ in range(10):\n",
    "            h = int(h * r)  # height to resize\n",
    "            img = pad(minmax_size(input_image.resize((w, h), Image.BILINEAR if r > 1 else Image.LANCZOS), args.max_dimensions, args.min_dimensions))\n",
    "            t = test_transform(image=np.array(img.convert('RGB')))['image'][:1].unsqueeze(0)\n",
    "            w = (image_resizer(t.to(args.device)).argmax(-1).item()+1)*32\n",
    "            logging.info(r, img.size, (w, int(input_image.size[1]*r)))\n",
    "            if (w == img.size[0]):\n",
    "                break\n",
    "            r = w/img.size[0]\n",
    "else:\n",
    "    img = np.array(pad(img).convert('RGB'))\n",
    "    t = test_transform(image=img)['image'][:1].unsqueeze(0)\n",
    "im = t.to(args.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    device = args.device\n",
    "    encoded = encoder(im.to(device))\n",
    "    dec = decoder.generate(torch.LongTensor([args.bos_token])[:, None].to(device), args.max_seq_len,\n",
    "                           eos_token=args.eos_token, context=encoded.detach(), temperature=args.get('temperature', .25))\n",
    "    pred = post_process(token2str(dec, tokenizer)[0])\n",
    "try:\n",
    "    clipboard.copy(pred)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\left\\\\{\\\\begin{array}{r c l}{{\\\\delta_{\\\\epsilon}B}}&{{\\\\sim}}&{{\\\\epsilon F\\\\,,}}\\\\\\\\ {{\\\\delta_{\\\\epsilon}F}}&{{\\\\sim}}&{{\\\\partial\\\\epsilon+\\\\epsilon B\\\\,,}}\\\\end{array}\\\\right.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\left\\\\{\\\\begin{array}{r c l}{{\\\\delta_{\\\\epsilon}B}}&{{\\\\sim}}&{{\\\\epsilon F\\\\,,}}\\\\\\\\ {{\\\\delta_{\\\\epsilon}F}}&{{\\\\sim}}&{{\\\\partial\\\\epsilon+\\\\epsilon B\\\\,,}}\\\\end{array}\\\\right.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pred.replace('<', '\\\\lt ').replace('>', '\\\\gt ')\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\left\\\\{\\\\begin{array}{r c l}{{\\\\delta_{\\\\epsilon}B}}&{{\\\\sim}}&{{\\\\epsilon F\\\\,,}}\\\\\\\\ {{\\\\delta_{\\\\epsilon}F}}&{{\\\\sim}}&{{\\\\partial\\\\epsilon+\\\\epsilon B\\\\,,}}\\\\end{array}\\\\right.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = str('\\\\left\\\\{\\\\begin{array}{r c l}{{\\\\delta_{\\\\epsilon}B}}&{{\\\\sim}}&{{\\\\epsilon F\\\\,,}}\\\\\\\\ {{\\\\delta_{\\\\epsilon}F}}&{{\\\\sim}}&{{\\\\partial\\\\epsilon+\\\\epsilon B\\\\,,}}\\\\end{array}\\\\right.')\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left\\{\\begin{array}{r c l}{{\\delta_{\\epsilon}B}}&{{\\sim}}&{{\\epsilon F\\,,}}\\\\ {{\\delta_{\\epsilon}F}}&{{\\sim}}&{{\\partial\\epsilon+\\epsilon B\\,,}}\\end{array}\\right.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html)\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageSource = \"\"\"\n",
    "        <html>\n",
    "        <head><script id=\"MathJax-script\" src=\"qrc:MathJax.js\"></script>\n",
    "        <script>\n",
    "        MathJax.Hub.Config({messageStyle: 'none',tex2jax: {preview: 'none'}});\n",
    "        MathJax.Hub.Queue(\n",
    "            function () {\n",
    "                document.getElementById(\"equation\").style.visibility = \"\";\n",
    "            }\n",
    "            );\n",
    "        </script>\n",
    "        </head> \"\"\" + \"\"\"\n",
    "        <body>\n",
    "        <div id=\"equation\" style=\"font-size:1em; visibility:hidden\">$${equation}$$</div>\n",
    "        </body>\n",
    "        </html>\n",
    "            \"\"\".format(equation=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <html>\n",
       "        <head><script id=\"MathJax-script\" src=\"qrc:MathJax.js\"></script>\n",
       "        <script>\n",
       "        MathJax.Hub.Config({messageStyle: 'none',tex2jax: {preview: 'none'}});\n",
       "        MathJax.Hub.Queue(\n",
       "            function () {\n",
       "                document.getElementById(\"equation\").style.visibility = \"\";\n",
       "            }\n",
       "            );\n",
       "        </script>\n",
       "        </head> \n",
       "        <body>\n",
       "        <div id=\"equation\" style=\"font-size:1em; visibility:hidden\">$$\\left\\{\\begin{array}{r c l}{{\\delta_{\\epsilon}B}}&{{\\sim}}&{{\\epsilon F\\,,}}\\\\ {{\\delta_{\\epsilon}F}}&{{\\sim}}&{{\\partial\\epsilon+\\epsilon B\\,,}}\\end{array}\\right.$$</div>\n",
       "        </body>\n",
       "        </html>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(pageSource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
