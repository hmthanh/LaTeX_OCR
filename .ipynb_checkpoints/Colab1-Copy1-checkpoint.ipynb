{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62b7b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnetv2 import ResNetV2\n",
    "from models.layers.std_conv import StdConv2dSame\n",
    "from models.vision_transformer import VisionTransformer\n",
    "from models.vision_transformer_hybrid import HybridEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7d151f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.autoregressive_wrapper import AutoregressiveWrapper, top_k, top_p, entmax, ENTMAX_ALPHA\n",
    "from models.x_transformers import TransformerWrapper, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "666cf613",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '__version__' from 'models.transformers.utils' (/home/lap14784/Downloads/LaTeX_OCR/models/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_202719/4044256302.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_fast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/LaTeX_OCR/models/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecoder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDecoderFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvert_slow_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m from .tokenization_utils_base import (\n",
      "\u001b[0;32m~/Downloads/LaTeX_OCR/models/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordPiece\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequires_backends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/LaTeX_OCR/models/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .hub import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mCLOUDFRONT_DISTRIB_PREFIX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mDISABLE_TELEMETRY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/LaTeX_OCR/models/transformers/utils/hub.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .import_utils import (\n\u001b[1;32m     29\u001b[0m     \u001b[0mENV_VARS_TRUE_VALUES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '__version__' from 'models.transformers.utils' (/home/lap14784/Downloads/LaTeX_OCR/models/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from models.transformers.tokenization_utils_fast import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e11490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from entmax import entmax_bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ffda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "from collections import namedtuple\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from entmax import entmax15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14b2576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4071a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas.io.clipboard as clipboard\n",
    "from PIL import ImageGrab\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from munch import Munch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from dataset.dataset import test_transform\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669b9109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x64 at 0x7F43B4351750>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# img = Image.open(\"./dataset/sample/1000a29807.png\")\n",
    "img = Image.open(\"./dataset/sample/10024a5ccf.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f12afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# from x_transformers import *\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class CustomARWrapper(AutoregressiveWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomARWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, start_tokens, seq_len, eos_token=None, temperature=1., filter_logits_fn=top_k, filter_thres=0.9, **kwargs):\n",
    "        device = start_tokens.device\n",
    "        was_training = self.net.training\n",
    "        num_dims = len(start_tokens.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            start_tokens = start_tokens[None, :]\n",
    "\n",
    "        b, t = start_tokens.shape\n",
    "\n",
    "        self.net.eval()\n",
    "        out = start_tokens\n",
    "        mask = kwargs.pop('mask', None)\n",
    "        if mask is None:\n",
    "            mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            x = out[:, -self.max_seq_len:]\n",
    "            mask = mask[:, -self.max_seq_len:]\n",
    "            # print('arw:',out.shape)\n",
    "            logits = self.net(x, mask=mask, **kwargs)[:, -1, :]\n",
    "\n",
    "            if filter_logits_fn in {top_k, top_p}:\n",
    "                filtered_logits = filter_logits_fn(logits, thres=filter_thres)\n",
    "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "\n",
    "            elif filter_logits_fn is entmax:\n",
    "                probs = entmax(logits / temperature, alpha=ENTMAX_ALPHA, dim=-1)\n",
    "\n",
    "            sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim=-1)\n",
    "            mask = F.pad(mask, (0, 1), value=True)\n",
    "\n",
    "            if eos_token is not None and (torch.cumsum(out == eos_token, 1)[:, -1] >= 1).all():\n",
    "                break\n",
    "\n",
    "        out = out[:, t:]\n",
    "\n",
    "        if num_dims == 1:\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        self.net.train(was_training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CustomVisionTransformer(VisionTransformer):\n",
    "    def __init__(self, img_size=224, patch_size=16, *args, **kwargs):\n",
    "        super(CustomVisionTransformer, self).__init__(img_size=img_size, patch_size=patch_size, *args, **kwargs)\n",
    "        self.height, self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, c, h, w = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        h, w = h//self.patch_size, w//self.patch_size\n",
    "        pos_emb_ind = repeat(torch.arange(h)*(self.width//self.patch_size-w), 'h -> (h w)', w=w)+torch.arange(h*w)\n",
    "        pos_emb_ind = torch.cat((torch.zeros(1), pos_emb_ind+1), dim=0).long()\n",
    "        x += self.pos_embed[:, pos_emb_ind]\n",
    "        #x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder: CustomVisionTransformer, decoder: CustomARWrapper, args, temp: float = .333):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.bos_token = args.bos_token\n",
    "        self.eos_token = args.eos_token\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.temperature = temp\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        device = x.device\n",
    "        encoded = self.encoder(x.to(device))\n",
    "        dec = self.decoder.generate(torch.LongTensor([self.bos_token]*len(x))[:, None].to(device), self.max_seq_len,\n",
    "                                    eos_token=self.eos_token, context=encoded, temperature=self.temperature)\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599f4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pic = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef1364b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lap14784/Downloads/LaTeX_OCR'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/lap14784/Downloads/LaTeX_OCR\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e019704",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = None\n",
    "if arguments is None:\n",
    "    arguments = Munch({'config': 'settings/config.yaml', 'checkpoint': 'checkpoints/weights.pth', 'no_cuda': True, 'no_resize': False})\n",
    "logging.getLogger().setLevel(logging.FATAL)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "with open(arguments.config, 'r') as f:\n",
    "    params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "args = parse_args(Munch(params))\n",
    "args.update(**vars(arguments))\n",
    "args.wandb = False\n",
    "# args.device = \"cpu\"\n",
    "args.device = 'cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54761cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ResNetV2(\n",
    "    layers=args.backbone_layers, num_classes=0, global_pool='', in_chans=args.channels,\n",
    "    preact=False, stem_type='same', conv_layer=StdConv2dSame)\n",
    "min_patch_size = 2**(len(args.backbone_layers)+1)\n",
    "\n",
    "def embed_layer(**x):\n",
    "    ps = x.pop('patch_size', min_patch_size)\n",
    "    assert ps % min_patch_size == 0 and ps >= min_patch_size, 'patch_size needs to be multiple of %i with current backbone configuration' % min_patch_size\n",
    "    return HybridEmbed(**x, patch_size=ps//min_patch_size, backbone=backbone)\n",
    "\n",
    "encoder = CustomVisionTransformer(img_size=(args.max_height, args.max_width),\n",
    "                                  patch_size=args.patch_size,\n",
    "                                  in_chans=args.channels,\n",
    "                                  num_classes=0,\n",
    "                                  embed_dim=args.dim,\n",
    "                                  depth=args.encoder_depth,\n",
    "                                  num_heads=args.heads,\n",
    "                                  embed_layer=embed_layer\n",
    "                                  ).to(args.device)\n",
    "\n",
    "decoder = CustomARWrapper(\n",
    "    TransformerWrapper(\n",
    "        num_tokens=args.num_tokens,\n",
    "        max_seq_len=args.max_seq_len,\n",
    "        attn_layers=Decoder(\n",
    "            dim=args.dim,\n",
    "            depth=args.num_layers,\n",
    "            heads=args.heads,\n",
    "            **args.decoder_args\n",
    "        )),\n",
    "    pad_value=args.pad_token\n",
    ").to(args.device)\n",
    "    wandb.watch((encoder, decoder.net.attn_layers))\n",
    "model = Model(encoder, decoder, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "824b45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_size(img, max_dimensions=None, min_dimensions=None):\n",
    "    if max_dimensions is not None:\n",
    "        ratios = [a/b for a, b in zip(img.size, max_dimensions)]\n",
    "        if any([r > 1 for r in ratios]):\n",
    "            size = np.array(img.size)//max(ratios)\n",
    "            img = img.resize(size.astype(int), Image.BILINEAR)\n",
    "    if min_dimensions is not None:\n",
    "        if any([s < min_dimensions[i] for i, s in enumerate(img.size)]):\n",
    "            padded_im = Image.new('L', min_dimensions, 255)\n",
    "            padded_im.paste(img, img.getbbox())\n",
    "            img = padded_im\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c39178fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments = Munch({'epoch': 0, 'backbone_layers': [2, 3, 7], 'betas': [0.9, 0.999], 'batchsize': 10, 'bos_token': 1, 'channels': 1, 'data': 'dataset/data/train.pkl', 'debug': False, 'decoder_args': {'attn_on_attn': True, 'cross_attend': True, 'ff_glu': True, 'rel_pos_bias': False, 'use_scalenorm': False}, 'dim': 256, 'encoder_depth': 4, 'eos_token': 2, 'epochs': 10, 'gamma': 0.9995, 'heads': 8, 'id': None, 'load_chkpt': None, 'lr': 0.001, 'lr_step': 30, 'max_height': 192, 'max_seq_len': 512, 'max_width': 672, 'min_height': 32, 'min_width': 32, 'model_path': 'checkpoints', 'name': 'pix2tex', 'num_layers': 4, 'num_tokens': 8000, 'optimizer': 'Adam', 'output_path': 'outputs', 'pad': False, 'pad_token': 0, 'patch_size': 16, 'sample_freq': 3000, 'save_freq': 5, 'scheduler': 'StepLR', 'seed': 42, 'temperature': 0.2, 'test_samples': 5, 'testbatchsize': 20, 'tokenizer': 'dataset/tokenizer.json', 'valbatches': 100, 'valdata': 'dataset/data/val.pkl', 'wandb': False, 'device': 'cpu', 'max_dimensions': [672, 192], 'min_dimensions': [32, 32], 'out_path': 'checkpoints/pix2tex', 'config': 'settings/config.yaml', 'checkpoint': 'checkpoints/weights.pth', 'no_cuda': False, 'no_resize': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf1ac292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n",
    "\n",
    "if 'image_resizer.pth' in os.listdir(os.path.dirname(args.checkpoint)) and not arguments.no_resize:\n",
    "    image_resizer = ResNetV2(layers=[2, 3, 3], num_classes=max(args.max_dimensions)//32, global_pool='avg', in_chans=1, drop_rate=.05,\n",
    "                             preact=True, stem_type='same', conv_layer=StdConv2dSame).to(args.device)\n",
    "    image_resizer.load_state_dict(torch.load(os.path.join(os.path.dirname(args.checkpoint), 'image_resizer.pth'), map_location=args.device))\n",
    "    image_resizer.eval()\n",
    "else:\n",
    "    image_resizer = None\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=args.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9b85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47467447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 160])\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = model.encoder, model.decoder\n",
    "if type(img) is bool:\n",
    "    img = None\n",
    "if img is None:\n",
    "    if last_pic is None:\n",
    "        print('Provide an image.')\n",
    "    else:\n",
    "        img = last_pic.copy()\n",
    "else:\n",
    "    last_pic = img.copy()\n",
    "img = minmax_size(pad(img), args.max_dimensions, args.min_dimensions)\n",
    "if image_resizer is not None and not args.no_resize:\n",
    "    with torch.no_grad():\n",
    "        input_image = img.convert('RGB').copy()\n",
    "        r, w, h = 1, input_image.size[0], input_image.size[1]\n",
    "        for _ in range(10):\n",
    "            h = int(h * r)  # height to resize\n",
    "            img = pad(minmax_size(input_image.resize((w, h), Image.BILINEAR if r > 1 else Image.LANCZOS), args.max_dimensions, args.min_dimensions))\n",
    "            t = test_transform(image=np.array(img.convert('RGB')))['image'][:1].unsqueeze(0)\n",
    "            w = (image_resizer(t.to(args.device)).argmax(-1).item()+1)*32\n",
    "            logging.info(r, img.size, (w, int(input_image.size[1]*r)))\n",
    "            if (w == img.size[0]):\n",
    "                break\n",
    "            r = w/img.size[0]\n",
    "else:\n",
    "    img = np.array(pad(img).convert('RGB'))\n",
    "    t = test_transform(image=img)['image'][:1].unsqueeze(0)\n",
    "im = t.to(args.device)\n",
    "print(np.shape(im))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    device = args.device\n",
    "    encoded = encoder(im.to(device))\n",
    "    dec = decoder.generate(torch.LongTensor([args.bos_token])[:, None].to(device), args.max_seq_len,\n",
    "                           eos_token=args.eos_token, context=encoded.detach(), temperature=args.get('temperature', .25))\n",
    "    pred = post_process(token2str(dec, tokenizer)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c4c1bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V_{2}(x)=-{\\\\frac{1}{2}}x^{2}-{\\\\frac{\\\\mu^{2}}{2x^{2}}},'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d5b89b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <html>\n",
       "        <head><script id=\"MathJax-script\" src=\"qrc:MathJax.js\"></script>\n",
       "        <script>\n",
       "        MathJax.Hub.Config({messageStyle: 'none',tex2jax: {preview: 'none'}});\n",
       "        MathJax.Hub.Queue(\n",
       "            function () {\n",
       "                document.getElementById(\"equation\").style.visibility = \"\";\n",
       "            }\n",
       "            );\n",
       "        </script>\n",
       "        </head> \n",
       "        <body>\n",
       "        <div id=\"equation\" style=\"font-size:1em; visibility:hidden\">$$V_{2}(x)=-{\\frac{1}{2}}x^{2}-{\\frac{\\mu^{2}}{2x^{2}}},$$</div>\n",
       "        </body>\n",
       "        </html>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pageSource = \"\"\"\n",
    "        <html>\n",
    "        <head><script id=\"MathJax-script\" src=\"qrc:MathJax.js\"></script>\n",
    "        <script>\n",
    "        MathJax.Hub.Config({messageStyle: 'none',tex2jax: {preview: 'none'}});\n",
    "        MathJax.Hub.Queue(\n",
    "            function () {\n",
    "                document.getElementById(\"equation\").style.visibility = \"\";\n",
    "            }\n",
    "            );\n",
    "        </script>\n",
    "        </head> \"\"\" + \"\"\"\n",
    "        <body>\n",
    "        <div id=\"equation\" style=\"font-size:1em; visibility:hidden\">$${equation}$$</div>\n",
    "        </body>\n",
    "        </html>\n",
    "            \"\"\".format(equation=pred)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(pageSource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262edc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584dd2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303e402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825bf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613fcca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f05f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
